#!/usr/bin/env python3
import asyncio
import html
import logging
import os
import pathlib
import re
from collections import defaultdict
from contextlib import suppress
from datetime import datetime, time, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple, Iterable, AsyncIterator
from urllib.parse import quote, urlparse

import aiosqlite
import httpx
import pytz
from dateutil import parser as dateparser
from dotenv import load_dotenv
from fastapi import FastAPI, Request, Query
from fastapi.responses import HTMLResponse
from starlette.concurrency import run_in_threadpool
from starlette.templating import Jinja2Templates
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.constants import ParseMode
from telegram.ext import (
    Application,
    ApplicationBuilder,
    CommandHandler,
    ContextTypes,
    JobQueue,
)
from telegram.helpers import escape_markdown

# =========================
# ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¸ ÑƒÑ‚Ð¸Ð»Ð¸Ñ‚Ñ‹
# =========================

load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN", "").strip()
API_BASE_URL = os.getenv("API_BASE_URL", "").rstrip("/")
API_SEARCH_PATH = "/events/search"
TIMEZONE_NAME = os.getenv("TIMEZONE", "Europe/Moscow")
LOCAL_TZ = pytz.timezone(TIMEZONE_NAME)
PUBLIC_BASE_URL = os.getenv("PUBLIC_BASE_URL", "").rstrip("/")
SSL_CERT_PATH = os.getenv("SSL_CERT_PATH", "/etc/certs/ssl.pem")
SSL_KEY_PATH = os.getenv("SSL_KEY_PATH", "/etc/certs/ssl.key")
DATABASE_PATH = os.getenv("DATABASE_PATH", "/data/chats.db")

_raw_hdr = os.getenv("API_AUTH_HEADER", "").strip()
API_HEADERS: Dict[str, str] = {}
if _raw_hdr:
    for part in _raw_hdr.split(";"):
        if ":" in part:
            k, v = part.split(":", 1)
            API_HEADERS[k.strip()] = v.strip()

QR_IDS_ENV = [s.strip() for s in os.getenv("QR_IDS", "").split(",") if
              s.strip()]

EVENT_VISIT = {"Link", "Visit"}
EVENT_SUB = {"Subscribe", "Subscription", "Follow"}
EVENT_UNSUB = {"Unsubscribe", "Unfollow"}

STATS_RE = re.compile(
    r"^/stats\s+(\S+)\s+(\d{2}\.\d{2}\.\d{4}\s+\d{2}:\d{2})\s*-\s*(\d{2}\.\d{2}\.\d{4}\s+\d{2}:\d{2})$",
    re.IGNORECASE,
)
STATS_TODAY_RE = re.compile(r"^/stats\s+(\S+)$", re.IGNORECASE)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
)
log = logging.getLogger("qr-bot+api")

templates = Jinja2Templates(directory="templates")
templates.env.globals["EVENT_VISIT"] = EVENT_VISIT
templates.env.globals["EVENT_SUB"] = EVENT_SUB
templates.env.globals["EVENT_UNSUB"] = EVENT_UNSUB


def ensure_config():
    if not BOT_TOKEN:
        raise RuntimeError("BOT_TOKEN is not set")
    if not API_BASE_URL:
        raise RuntimeError("API_BASE_URL is not set")


def parse_local_dt(s: str) -> datetime:
    dt = datetime.strptime(s, "%d.%m.%Y %H:%M")
    return LOCAL_TZ.localize(dt)


def fmt_local(dt: datetime) -> str:
    dt_loc = dt.astimezone(LOCAL_TZ)
    return dt_loc.strftime("%d.%m.%Y %H:%M")


LOCAL_HOSTS = {"localhost", "127.0.0.1", "0.0.0.0"}


def is_localhost_url(url: str) -> bool:
    try:
        host = (urlparse(url).hostname or "").lower()
        return host in LOCAL_HOSTS
    except Exception:
        return False


def make_markdown_link(text: str, url: str) -> str:
    # Ð½Ðµ ÑÐºÑ€Ð°Ð½Ð¸Ñ€ÑƒÐµÐ¼ text â€” Ð¾Ð½ ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ "ÐŸÐ¾Ð»Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°"
    # url ÑƒÐ¶Ðµ percent-encoded Ð² build_stats_link
    return f"[{text}]({url})"


templates.env.globals["fmt_local"] = fmt_local


def to_iso(dt: datetime) -> str:
    return (
        dt.astimezone(timezone.utc)
        .isoformat(timespec="milliseconds")
        .replace("+00:00", "Z")
    )


def build_stats_link(qr_id: str, dt_from: datetime, dt_to: datetime) -> str:
    if not PUBLIC_BASE_URL:
        return ""
    base = PUBLIC_BASE_URL
    if not base.startswith(("http://", "https://")):
        base = "http://" + base
    f_iso = to_iso(dt_from)
    t_iso = to_iso(dt_to)
    f_q = quote(f_iso, safe="")
    t_q = quote(t_iso, safe="")
    return f"{base}/stats/{qr_id}?from={f_q}&to={t_q}"


# =========================
# Async HTTP ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ðº stats
# =========================

_http_client: Optional[httpx.AsyncClient] = None


async def api_search_async(
    qr_id: Optional[str],
    dt_from: datetime,
    dt_to: datetime,
    *,
    limit: int = 100,
    start_cursor: Optional[str] = None,
    max_pages: Optional[int] = None,
    max_events: Optional[int] = None,
) -> AsyncIterator[Dict[str, Any]]:
    """
    ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ (dict) Ð¸Ð· API, Ñ Ð°Ð²Ñ‚Ð¾Ð¿Ð°Ð³Ð¸Ð½Ð°Ñ†Ð¸ÐµÐ¹.
    """
    assert _http_client is not None, "HTTP client not initialized"
    url = f"{API_BASE_URL}{API_SEARCH_PATH}"

    cursor = start_cursor
    total_events = 0
    page_count = 0

    while True:
        payload: Dict[str, Any] = {
            "from": to_iso(dt_from),
            "to": to_iso(dt_to),
            "qr_id": qr_id,
            "limit": limit,
        }
        if cursor is not None:
            payload["cursor"] = cursor

        log.debug("POST %s payload=%s headers=%s", url, payload, API_HEADERS)
        r = await _http_client.post(
            url, json=payload, headers=API_HEADERS, timeout=30
        )
        r.raise_for_status()
        data = r.json()

        if not isinstance(data, dict) or "events" not in data:
            raise ValueError(f"Unexpected API response shape: {data}")

        events = data.get("events", [])
        has_more = bool(data.get("has_more", False))
        cursor = data.get("next_cursor")

        log.debug(
            "Fetched page %d: %d events, has_more=%s, next_cursor=%s",
            page_count + 1, len(events), has_more, cursor,
        )

        for event in events:
            yield event
            total_events += 1
            if max_events is not None and total_events >= max_events:
                log.info("Reached max_events=%s; stopping", max_events)
                return

        page_count += 1
        if not has_more or not cursor:
            break
        if max_pages is not None and page_count >= max_pages:
            log.info("Reached max_pages=%s; stopping pagination", max_pages)
            break

    log.debug(
        "Completed fetching %d events in %d pages", total_events, page_count
    )


# =========================
# Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ (ÑÑ‚Ñ€Ð¸Ð¼-Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ñ)
# =========================

def escape_md(text: str) -> str:
    return escape_markdown(text, version=2)


def _inc_counts_for_event(
    event: Dict[str, Any], counts: Dict[str, int]
) -> None:
    et = event.get("event_type")
    if et in EVENT_VISIT:
        counts["visits"] += 1
    elif et in EVENT_SUB:
        counts["subs"] += 1
    elif et in EVENT_UNSUB:
        counts["unsubs"] += 1


def _add_event_to_users(
    users: Dict[int, Dict[str, Any]], event: Dict[str, Any]
) -> None:
    info = event.get("vk_user_info") or {}
    uid = info.get("id")
    if uid is None:
        return

    bucket = users.setdefault(
        uid,
        {
            "first_name": info.get("first_name") or "",
            "last_name": info.get("last_name") or "",
            "events": [],
            "subscribed_in_range": False,
            "link": f"https://vk.com/id{uid}",
        },
    )

    ts_raw = event.get("timestamp")
    try:
        ts = dateparser.isoparse(ts_raw)
        if ts.tzinfo is None:
            ts = ts.replace(tzinfo=timezone.utc)
    except Exception:
        return

    etype = event.get("event_type")
    payload = event.get("payload") or {}

    if etype in EVENT_SUB:
        bucket["subscribed_in_range"] = True

    bucket["events"].append((ts, etype, payload))


async def fold_users_and_counts(
    qr_id: Optional[str],
    dt_from: datetime,
    dt_to: datetime,
) -> Tuple[Dict[int, Dict[str, Any]], int, int, int, int]:
    """
    Ð¡Ñ‚Ñ€Ð¸Ð¼Ð¸Ñ‚ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¸ Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð¾ ÑÐ¾Ð±Ð¸Ñ€Ð°ÐµÑ‚:
    - users: ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° ÐºÐ°Ðº Ñ€Ð°Ð½ÑŒÑˆÐµ (Ñ events Ð¿Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼)
    - visits/subs/unsubs
    - events_count
    """
    users: Dict[int, Dict[str, Any]] = {}
    counts = {"visits": 0, "subs": 0, "unsubs": 0}
    events_count = 0

    async for ev in api_search_async(qr_id, dt_from, dt_to):
        _add_event_to_users(users, ev)
        _inc_counts_for_event(ev, counts)
        events_count += 1

    for b in users.values():
        b["events"].sort(key=lambda t: t[0])

    return users, counts["visits"], counts["subs"], counts[
        "unsubs"], events_count


async def any_event_with_qr(
    qr_id: str,
    dt_from: datetime,
    dt_to: datetime,
) -> bool:
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ ÑÑ‚Ñ€Ð¸Ð¼Ð¸Ð½Ð³Ð¾Ð¼, Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ð»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ qr_id Ð² ÑÐ¾Ð±Ñ‹Ñ‚Ð¸ÑÑ… Ð·Ð° Ð¿ÐµÑ€Ð¸Ð¾Ð´."""
    qr_l = qr_id.lower()
    async for ev in api_search_async(qr_id, dt_from, dt_to, limit=500):
        payload = ev.get("payload") or {}
        ev_qr = (payload.get("qr_id") or payload.get("qrId") or "").lower()
        if ev_qr == qr_l:
            return True
    return False


async def summarize_counts_by_qr_stream(
    dt_from: datetime,
    dt_to: datetime,
    qr_id: Optional[str] = None,
) -> Dict[str, Tuple[int, int, int]]:
    """
    ÐŸÐ¾Ñ‚Ð¾ÐºÐ¾Ð²Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¾Ð³ summarize_counts_by_qr: Ð±ÐµÐ· Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð²ÑÐµÑ… events.
    """
    buckets = defaultdict(lambda: [0, 0, 0])  # [visits, subs, unsubs]
    async for ev in api_search_async(qr_id, dt_from, dt_to):
        q = ev.get("payload", {}).get("qr_id")
        if q is None:
            continue
        et = ev.get("event_type")
        if et in EVENT_VISIT:
            buckets[q][0] += 1
        elif et in EVENT_SUB:
            buckets[q][1] += 1
        elif et in EVENT_UNSUB:
            buckets[q][2] += 1
    return {q: (v[0], v[1], v[2]) for q, v in buckets.items()}


def build_summary_message_stream_markdown(
    qr_id: str,
    dt_from: datetime,
    dt_to: datetime,
    visits: int,
    subs: int,
    unsubs: int,
    link_markdown: str | None = None,
) -> str:
    lines = [
        f"ðŸ“ *{escape_md(qr_id.upper().replace('QR', 'QR-'))}*",
        f"ðŸ“… ÐŸÐµÑ€Ð¸Ð¾Ð´: {escape_md(fmt_local(dt_from))} â€“ {escape_md(fmt_local(dt_to))}",
        "",
        "*Ð˜Ñ‚Ð¾Ð³:*",
        f"ÐŸÐ¾ÑÐµÑ‰ÐµÐ½Ð¸Ð¹: *{visits}*",
        f"ÐŸÐ¾Ð´Ð¿Ð¸ÑÐ¾Ðº: *{subs}*",
        f"ÐžÑ‚Ð¿Ð¸ÑÐ¾Ðº: *{unsubs}*",
    ]
    if link_markdown:
        lines.append("")
        # Ð²Ð°Ð¶Ð½Ð¾: Ð½Ðµ Ð¿Ñ€Ð¾Ð³Ð¾Ð½ÑÑ‚ÑŒ Ñ‡ÐµÑ€ÐµÐ· escape_md â€” Ð¸Ð½Ð°Ñ‡Ðµ ÑÐ»Ð¾Ð¼Ð°ÐµÐ¼ ÑÐ¸Ð½Ñ‚Ð°ÐºÑÐ¸Ñ ÑÑÑ‹Ð»ÐºÐ¸
        lines.append(link_markdown)
    return "\n".join(lines)


def build_summary_message_stream_plain(
    qr_id: str,
    dt_from: datetime,
    dt_to: datetime,
    visits: int,
    subs: int,
    unsubs: int,
    url: str | None = None,
) -> str:
    lines = [
        f"ðŸ“ {qr_id.upper().replace('QR', 'QR-')}",
        f"ðŸ“… ÐŸÐµÑ€Ð¸Ð¾Ð´: {fmt_local(dt_from)} â€“ {fmt_local(dt_to)}",
        "",
        "Ð˜Ñ‚Ð¾Ð³:",
        f"ÐŸÐ¾ÑÐµÑ‰ÐµÐ½Ð¸Ð¹: {visits}",
        f"ÐŸÐ¾Ð´Ð¿Ð¸ÑÐ¾Ðº: {subs}",
        f"ÐžÑ‚Ð¿Ð¸ÑÐ¾Ðº: {unsubs}",
    ]
    if url:
        lines.append("")
        lines.append(url)  # Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Â«ÐºÐ°Ðº ÐµÑÑ‚ÑŒÂ»
    return "\n".join(lines)


def split_message(s: str, limit: int = 4000) -> Iterable[str]:
    if len(s) <= limit:
        return [s]
    parts = []
    buf = []
    ln = 0
    for line in s.splitlines():
        if ln + len(line) + 1 > limit and buf:
            parts.append("\n".join(buf))
            buf = []
            ln = 0
        buf.append(line)
        ln += len(line) + 1
    if buf:
        parts.append("\n".join(buf))
    return parts


# =========================
# FastAPI (async)
# =========================

class EventOutDict(Dict[str, Any]):
    ...


class UserOutDict(Dict[str, Any]):
    ...


class StatsOutDict(Dict[str, Any]):
    ...


app = FastAPI(title="QR Stats API")


@app.get("/health")
async def health():
    return {"ok": True}


@app.get(
    "/stats/{qr_id}", response_class=HTMLResponse
)
async def http_stats(
    qr_id: str,
    request: Request,
    from_iso: str = Query(..., alias="from"),
    to_iso: str = Query(..., alias="to"),
):
    try:
        dt_from = dateparser.isoparse(from_iso)
        dt_to = dateparser.isoparse(to_iso)
        if dt_from.tzinfo is None:
            dt_from = LOCAL_TZ.localize(dt_from)
        if dt_to.tzinfo is None:
            dt_to = LOCAL_TZ.localize(dt_to)
        if dt_to <= dt_from:
            raise ValueError("Invalid period")
    except Exception as e:
        return templates.TemplateResponse(
            "error.html",
            {"request": request, "title": "ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ð´Ð°Ñ‚Ðµ", "message": str(e)},
            status_code=400,
        )

    try:
        users, visits, subs, unsubs, events_count = await fold_users_and_counts(
            qr_id, dt_from, dt_to
        )

        context = {
            "request": request,
            "qr_id": qr_id,
            "dt_from": dt_from,
            "dt_to": dt_to,
            "fmt_from": fmt_local(dt_from),
            "fmt_to": fmt_local(dt_to),
            "visits": visits,
            "subs": subs,
            "unsubs": unsubs,
            "users": users,
            "events_count": events_count,
        }
        # Jinja Ñ€ÐµÐ½Ð´ÐµÑ€ â€” ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹; Ð²Ñ‹Ð½Ð¾ÑÐ¸Ð¼ Ð² Ð¿Ð¾Ñ‚Ð¾Ðº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ loop
        return await run_in_threadpool(
            templates.TemplateResponse, "stats.html", context
        )

    except httpx.HTTPError as e:
        return templates.TemplateResponse(
            "error.html",
            {"request": request, "title": "ÐžÑˆÐ¸Ð±ÐºÐ° API", "message": str(e)},
            status_code=502,
        )
    except Exception as e:
        log.exception("http_stats failed")
        return templates.TemplateResponse(
            "error.html",
            {"request": request, "title": "ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐµÑ€Ð²ÐµÑ€Ð°", "message": str(e)},
            status_code=500,
        )


# =========================
# Ð‘Ð” (aiosqlite)
# =========================

DB_PATH = str(pathlib.Path(DATABASE_PATH).resolve())
pathlib.Path(DB_PATH).parent.mkdir(parents=True, exist_ok=True)


async def init_db():
    async with aiosqlite.connect(DB_PATH) as conn:
        await conn.execute(
            """
            CREATE TABLE IF NOT EXISTS digest_chats (
                chat_id INTEGER PRIMARY KEY,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """
        )
        await conn.commit()


async def add_chat_async(chat_id: int):
    async with aiosqlite.connect(DB_PATH) as conn:
        await conn.execute(
            "INSERT OR IGNORE INTO digest_chats(chat_id) VALUES (?)", (chat_id,)
        )
        await conn.commit()


async def remove_chat_async(chat_id: int):
    async with aiosqlite.connect(DB_PATH) as conn:
        await conn.execute(
            "DELETE FROM digest_chats WHERE chat_id = ?", (chat_id,)
        )
        await conn.commit()


async def list_chats_async() -> list[int]:
    async with aiosqlite.connect(DB_PATH) as conn:
        cur = await conn.execute(
            "SELECT chat_id FROM digest_chats ORDER BY chat_id"
        )
        rows = await cur.fetchall()
        return [r[0] for r in rows]


# =========================
# Telegram (async)
# =========================

async def stats_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    m = STATS_RE.match(text)
    if m:
        qr_id, from_s, to_s = m.groups()
        try:
            dt_from = parse_local_dt(from_s)
            dt_to = parse_local_dt(to_s)
            if dt_to <= dt_from:
                raise ValueError("ÐŸÐµÑ€Ð¸Ð¾Ð´ Ð¿ÑƒÑÑ‚Ð¾Ð¹")
        except Exception:
            await update.message.reply_text(
                "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ Ð´Ð°Ñ‚Ñ‹. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ DD.MM.YYYY HH:MM."
            )
            return
    else:
        # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ð±ÐµÐ· Ð´Ð°Ñ‚Ñ‹ (/stats qr1) â€” Ð·Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ
        m_today = STATS_TODAY_RE.match(text)
        if m_today:
            qr_id = m_today.group(1)
            now_loc = datetime.now(LOCAL_TZ)
            dt_from = now_loc.replace(hour=0, minute=0, second=0, microsecond=0)
            dt_to = now_loc
        else:
            await update.message.reply_text(
                "ÐÐµÐ²ÐµÑ€Ð½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚. ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:\n"
                "/stats qr1 â€” ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð·Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ\n"
                "/stats qr1 07.09.2025 14:00 - 08.09.2025 11:00 â€” Ð·Ð° Ð¿ÐµÑ€Ð¸Ð¾Ð´"
            )
            return

    try:
        users, visits, subs, unsubs, events_count = await fold_users_and_counts(
            qr_id, dt_from, dt_to
        )

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ñ QR ÐµÑÐ»Ð¸ Ð¿ÑƒÑÑ‚Ð¾
        if events_count == 0:
            check_from = datetime.now(LOCAL_TZ) - timedelta(days=365)
            check_to = datetime.now(LOCAL_TZ)
            qr_found_ever = await any_event_with_qr(qr_id, check_from, check_to)
            if not qr_found_ever:
                await update.message.reply_text(
                    f"âŒ QR-ÐºÐ¾Ð´ '{qr_id}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ.\n"
                    f"ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ."
                )
                return

        link = build_stats_link(qr_id, dt_from, dt_to)

        if link and not is_localhost_url(link):
            link_md = make_markdown_link("ÐŸÐ¾Ð»Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°", link)
            msg = build_summary_message_stream_markdown(
                qr_id, dt_from, dt_to, visits, subs, unsubs,
                link_markdown=link_md
            )
            await update.message.reply_text(
                msg, parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True
            )
        else:
            # localhost / Ð¿ÑƒÑÑ‚Ð°Ñ ÑÑÑ‹Ð»ÐºÐ° â€” Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ plain text Â«ÐºÐ°Ðº ÐµÑÑ‚ÑŒÂ»
            msg = build_summary_message_stream_plain(
                qr_id, dt_from, dt_to, visits, subs, unsubs, url=link or ""
            )
            await update.message.reply_text(msg, disable_web_page_preview=True)

    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            await update.message.reply_text(
                f"âŒ QR-ÐºÐ¾Ð´ '{qr_id}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ."
            )
        else:
            log.exception("stats_cmd failed with HTTP error")
            await update.message.reply_text(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {e}")
    except Exception as e:
        log.exception("stats_cmd failed")
        await update.message.reply_text(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {e}")


async def start_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ ÑÑ‡Ð¸Ñ‚Ð°ÑŽ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¿Ð¾ QR.\n"
        "Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°:\n"
        "/stats qr1 â€” ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð·Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ\n"
        "/stats qr1 DD.MM.YYYY HH:MM - DD.MM.YYYY HH:MM â€” Ð·Ð° Ð¿ÐµÑ€Ð¸Ð¾Ð´\n"
        "ÐšÐ°Ð¶Ð´Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ Ð² 21:00 ÐœÐ¡Ðš Ñ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ ÑÐ²Ð¾Ð´ÐºÑƒ Ð¿Ð¾ Ð²ÑÐµÐ¼ QR (ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾)."
    )


async def daily_digest(context: ContextTypes.DEFAULT_TYPE):
    job = context.job
    chat_id = job.data.get("chat_id")

    now_loc = datetime.now(LOCAL_TZ)
    day_start = now_loc.replace(hour=0, minute=0, second=0, microsecond=0)
    day_end = day_start + timedelta(days=1) - timedelta(milliseconds=1)

    try:
        per_qr = await summarize_counts_by_qr_stream(
            day_start, day_end, qr_id=None
        )

        lines = ["Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ Ð²ÑÐµÐ¼ QR Ð·Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ.\n"]
        if per_qr:
            for qr_id, (visits, subs, unsubs) in per_qr.items():
                lines.append(f"{qr_id}:")
                lines.append(f"ÐŸÐ¾ÑÐµÑ‰ÐµÐ½Ð¸Ð¹: {visits}")
                lines.append(f"ÐŸÐ¾Ð´Ð¿Ð¸ÑÐ¾Ðº: {subs}")
                lines.append(f"ÐžÑ‚Ð¿Ð¸ÑÐ¾Ðº: {unsubs}\n")
        else:
            lines.append("Ð¡Ð¾Ð±Ñ‹Ñ‚Ð¸Ð¹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾.")
        msg = "\n".join(lines)

        for chunk in split_message(msg):
            await context.bot.send_message(chat_id=chat_id, text=chunk)
    except Exception as e:
        log.exception("daily_digest failed")
        await context.bot.send_message(
            chat_id=chat_id, text=f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸ ÑÐ²Ð¾Ð´ÐºÐ¸: {e}"
        )


async def enable_digest_for_chat(update, context):
    await add_chat_async(update.effective_chat.id)
    if not context.job_queue:
        await update.message.reply_text("â›”ï¸ ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸Ðº Ð½Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½.")
        return

    chat_id = update.effective_chat.id

    # ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ€ÑƒÑŽ job, ÐµÑÐ»Ð¸ Ð±Ñ‹Ð»Ð°
    for j in context.job_queue.get_jobs_by_name(f"digest_{chat_id}"):
        j.schedule_removal()

    # Ð¿Ð»Ð°Ð½Ð¸Ñ€ÑƒÐµÐ¼ (21:00 ÐœÐ¡Ðš = 00:00 ÐœÐ¡Ðš+3 â€” Ð¾ÑÑ‚Ð°Ð²Ð¸Ð¼ Ñ‚Ð²Ð¾Ð¸ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð°)
    context.job_queue.run_daily(
        callback=daily_digest,
        time=time(hour=21, minute=0, tzinfo=LOCAL_TZ),
        name=f"digest_{chat_id}",
        data={"chat_id": chat_id},
        job_kwargs={"misfire_grace_time": 300, "coalesce": True},
    )

    await update.message.reply_text("âœ… Ð•Ð¶ÐµÐ´Ð½ÐµÐ²Ð½Ð°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð° (21:00 ÐœÐ¡Ðš).")


async def disable_digest_for_chat(update, context):
    chat_id = update.effective_chat.id
    await remove_chat_async(chat_id)
    removed = 0
    if context.job_queue:
        for j in context.job_queue.get_jobs_by_name(f"digest_{chat_id}"):
            j.schedule_removal()
            removed += 1
    await update.message.reply_text(
        "ðŸ›‘ Ð•Ð¶ÐµÐ´Ð½ÐµÐ²Ð½Ð°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°."
        if removed or True
        else "â„¹ï¸ Ð¡Ð²Ð¾Ð´ÐºÐ° ÑƒÐ¶Ðµ Ð±Ñ‹Ð»Ð° Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°."
    )


_tg_app: Optional[Application] = None


async def _start_telegram():
    global _tg_app
    _tg_app = (
        ApplicationBuilder()
        .token(BOT_TOKEN)
        .job_queue(JobQueue())
        .build()
    )

    await init_db()
    for chat_id in await list_chats_async():
        _tg_app.job_queue.run_daily(
            callback=daily_digest,
            time=time(hour=21, minute=00, tzinfo=LOCAL_TZ),
            name=f"digest_{chat_id}",
            data={"chat_id": chat_id},
            job_kwargs={"misfire_grace_time": 300, "coalesce": True},
        )
        log.info("Restored daily digest for chat %s", chat_id)

    _tg_app.add_handler(CommandHandler("start", start_cmd, block=False))
    _tg_app.add_handler(CommandHandler("startdigest", enable_digest_for_chat))
    _tg_app.add_handler(CommandHandler("stopdigest", disable_digest_for_chat))
    _tg_app.add_handler(CommandHandler("stats", stats_cmd, block=False))

    await _tg_app.initialize()
    await _tg_app.start()
    await _tg_app.updater.start_polling(allowed_updates=Update.ALL_TYPES)
    log.info("Telegram bot started")


async def _stop_telegram():
    if _tg_app:
        await _tg_app.updater.stop()
        await _tg_app.stop()
        await _tg_app.shutdown()
        log.info("Telegram bot stopped")


@app.on_event("startup")
async def on_startup():
    ensure_config()
    global _http_client
    _http_client = httpx.AsyncClient()
    await _start_telegram()


@app.on_event("shutdown")
async def on_shutdown():
    await _stop_telegram()
    global _http_client
    if _http_client:
        await _http_client.aclose()
        _http_client = None
    tg = getattr(app.state, "tg_task", None)
    if tg:
        tg.cancel()
        with suppress(asyncio.CancelledError):
            await tg
    log.info("Shutdown complete")
